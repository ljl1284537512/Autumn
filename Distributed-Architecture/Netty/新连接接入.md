# 新连接接入

## 调试入口确定
首先，我们知道服务器这边配置了`ChannelHandler`之后，一定会执行里面的方法，以`EchoServer`为例, 我们将断点打在`ChannelHandler`里面：
```java
@ChannelHandler.Sharable
public class EchoServerMessageHandler extends ChannelInboundHandlerAdapter {
    @Override
    public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception {
        打断点的位置...
        ByteBuf in = (ByteBuf)msg;
        // 调用下一个inboundHandler处理
        System.out.println("第一个inBoundhandler获得处理");
        ctx.fireChannelRead(msg);
    }

    @Override
    public void channelReadComplete(ChannelHandlerContext ctx) throws Exception {
        // 调用下一个inboundHandler处理
        ctx.fireChannelReadComplete();
    }

    @Override
    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception {
        cause.printStackTrace();
        ctx.close();
    }
}
```

当线程执行到断点处，我们看调用的线程栈如下: 
![](_v_images/20191116221905246_12146.png =900x)
根据之前的了解，我们确认入口为`NioEventLoop.run()`方法，并且当前执行NioEventLoop是属于Boss线程组中的线程；

## 新连接接入过程
如上图所示，接下来会调用`processSelectedKeys:499`, 我们逐行尝试分析下流程：
```java
io.netty.channel.nio.NioEventLoop#processSelectedKeys
// 处理选择器选中的keys
private void processSelectedKeys() {
    if (selectedKeys != null) {
        processSelectedKeysOptimized();
    } else {
        processSelectedKeysPlain(selector.selectedKeys());
    }
}
```
seletedKeys其实是一个Set集合，如果没有新连接接入，那么这个Set为Empty， 内部无有效操作。

## 实例启动客户端，让新连接接入

接下来看`processSelectedKeysOptimized();`
```java
io.netty.channel.nio.NioEventLoop#processSelectedKeysOptimized
private void processSelectedKeysOptimized() {
    // 遍历所有选中的Keys
    for (int i = 0; i < selectedKeys.size; ++i) {
        final SelectionKey k = selectedKeys.keys[i];
        // null out entry in the array to allow to have it GC'ed once the Channel close
        // See https://github.com/netty/netty/issues/2363
        // 如果不置空的话，万一channel关闭了之后，这个selectedKeys内部还保留这个key的强引用，就无法做到垃圾回收
        // TODO 即使这个Channel不关闭，selectedKeys中的引用还能被复用吗？
        selectedKeys.keys[i] = null;

        final Object a = k.attachment();
        // a = NioSocketChannel实例
        if (a instanceof AbstractNioChannel) {
            // 如果是AbstractNioChannel的实例, 则进行如下处理：
            processSelectedKey(k, (AbstractNioChannel) a);
        } else {
            @SuppressWarnings("unchecked")
            NioTask<SelectableChannel> task = (NioTask<SelectableChannel>) a;
            processSelectedKey(k, task);
        }

        if (needsToSelectAgain) {
            // null out entries in the array to allow to have it GC'ed once the Channel close
            // See https://github.com/netty/netty/issues/2363
            selectedKeys.reset(i + 1);

            selectAgain();
            i = -1;
        }
    }
}
```
如果是AbstractNioChannel的实例, 则进行如下处理：`processSelectedKey(k, (AbstractNioChannel) a);`
```java
private void processSelectedKey(SelectionKey k, AbstractNioChannel ch) {
    ...
    try {
        int readyOps = k.readyOps();
        // We first need to call finishConnect() before try to trigger a read(...) or write(...) as otherwise
        // the NIO JDK channel implementation may throw a NotYetConnectedException.
        // 看解释，貌似要解决的问题是，如果不提前执行finsishConnect操作，那么后面的read或者write会报错。
        if ((readyOps & SelectionKey.OP_CONNECT) != 0) {
            // 回应OP_CONNECT事件
            // remove OP_CONNECT as otherwise Selector.select(..) will always return without blocking
            // See https://github.com/netty/netty/issues/924
            int ops = k.interestOps();
            ops &= ~SelectionKey.OP_CONNECT;
            k.interestOps(ops);

            unsafe.finishConnect();
        }

        // Process OP_WRITE first as we may be able to write some queued buffers and so free memory.
        if ((readyOps & SelectionKey.OP_WRITE) != 0) {
            // Call forceFlush which will also take care of clear the OP_WRITE once there is nothing left to write
            ch.unsafe().forceFlush();
        }

        // 回应OP_ACCEPT事件
        // Also check for readOps of 0 to workaround possible JDK bug which may otherwise lead
        // to a spin loop
        // 这块貌似和空循环有关系？
        if ((readyOps & (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) {
            unsafe.read();
        }
    } catch (CancelledKeyException ignored) {
        unsafe.close(unsafe.voidPromise());
    }
}
```
客户端启动之后，服务端接收到了OP_ACCEPT操作，执行unsafe.read()操作。

## 新连接进一步处理
对接收的新连接，执行read操作，这个时候仍然是bossGroup中的EventLoop在处理。
```java
io.netty.channel.nio.AbstractNioMessageChannel.NioMessageUnsafe#read
private final class NioMessageUnsafe extends AbstractNioUnsafe {
        private final List<Object> readBuf = new ArrayList<Object>();
        @Override
        public void read() {
            assert eventLoop().inEventLoop();
            final ChannelConfig config = config();
            final ChannelPipeline pipeline = pipeline();
            final RecvByteBufAllocator.Handle allocHandle = unsafe().recvBufAllocHandle();
            allocHandle.reset(config);

            boolean closed = false;
            Throwable exception = null;
            try {
                try {
                    do {
                        // 将创建的SocketChannel缓存到readBuf中
                        int localRead = doReadMessages(readBuf);
                        if (localRead == 0) {
                            break;
                        }
                        if (localRead < 0) {
                            closed = true;
                            break;
                        }
                        // 记录总共读取的消息总数
                        allocHandle.incMessagesRead(localRead);
                    } while (allocHandle.continueReading());
                } catch (Throwable t) {
                    exception = t;
                }
                int size = readBuf.size();
                for (int i = 0; i < size; i ++) {
                    readPending = false;
                    // 传播ChannelRead事件
                    pipeline.fireChannelRead(readBuf.get(i));
                }
                readBuf.clear();
                allocHandle.readComplete();
                pipeline.fireChannelReadComplete();

                if (exception != null) {
                    closed = closeOnReadError(exception);

                    pipeline.fireExceptionCaught(exception);
                }

                if (closed) {
                    inputShutdown = true;
                    if (isOpen()) {
                        close(voidPromise());
                    }
                }
            } finally {
                // Check if there is a readPending which was not processed yet.
                // This could be for two reasons:
                // * The user called Channel.read() or ChannelHandlerContext.read() in channelRead(...) method
                // * The user called Channel.read() or ChannelHandlerContext.read() in channelReadComplete(...) method
                //
                // See https://github.com/netty/netty/issues/2254
                if (!readPending && !config.isAutoRead()) {
                    removeReadOp();
                }
            }
        }
    }
```
pipeLine 在传播ChannelRead事件时，pipeline中的ChannelHandler构造： HeadContext -> ServerBootstrapAcceptor -> TailContext。HeadContext直接往下传播，而ServerBoostrapAcceptor在回调执行channelRead时，将当前的SocketChannel注册到了workerGroup中的EventLoop中。继而接下来的数据读取，都有workerEventLoop来处理。
接着传播channelReadComplete事件，HeadContext，ServerBootstrapAcceptor以及TailContext中，HeadContext的处理比较特殊。

### doReadMessage
通过ServerSocketChannel的accept接收客户端的连接请求并创建SocketChannel实例，完成以上操作之后，相当于完成了TCP的三次握手，TCP物理链路正式建立。
```java
@Override
    protected int doReadMessages(List<Object> buf) throws Exception {
        // javaChannel是从Channel中获取Netty内部封装的SocketChannel
        // 并执行ServerSocketChannel.accept()方法
        SocketChannel ch = SocketUtils.accept(javaChannel());
        try {
            if (ch != null) {
                // 将新创建的SocketChannel缓存起来
                buf.add(new NioSocketChannel(this, ch));
                return 1;
            }
        } catch (Throwable t) {
        }
        return 0;
    }
```
## 正式交互
连接建立之后，就会有消息的交互，当客户端在连接建立完成之后，发送消息给服务端，服务端接到新的信息（当然是由workerGroup中的NioEventLoop来处理channel中产生的事件）, 继续执行read操作，但是这回的read的执行是NioByteUnsafe来完成的。
```java
io.netty.channel.nio.AbstractNioByteChannel.NioByteUnsafe#read
@Override
public final void read() {
    final ChannelConfig config = config();
    if (shouldBreakReadReady(config)) {
        clearReadPending();
        return;
    }
    // 取出pipeline
    final ChannelPipeline pipeline = pipeline();
    // allocator = ByteBufAllocator.DEFAULT;
    final ByteBufAllocator allocator = config.getAllocator();
    final RecvByteBufAllocator.Handle allocHandle = recvBufAllocHandle();
    allocHandle.reset(config);

    ByteBuf byteBuf = null;
    boolean close = false;
    try {
        do {
            // 开辟缓冲区
            byteBuf = allocHandle.allocate(allocator);
            // 将字节读入到缓存区
            allocHandle.lastBytesRead(doReadBytes(byteBuf));
            if (allocHandle.lastBytesRead() <= 0) {
                // 如果此次没有数据需要读取，则需要释放缓冲区
                // nothing was read. release the buffer.
                byteBuf.release();
                byteBuf = null;
                close = allocHandle.lastBytesRead() < 0;
                if (close) {
                    // There is nothing left to read as we received an EOF.
                    readPending = false;
                }
                break;
            }

            allocHandle.incMessagesRead(1);
            readPending = false;
            // pipeline 传播channelRead操作
            pipeline.fireChannelRead(byteBuf);
            byteBuf = null;
        } while (allocHandle.continueReading());

        allocHandle.readComplete();
        //数据都读取完毕之后，pipeline 传播读数据结束操作
        pipeline.fireChannelReadComplete();

        if (close) {
            closeOnRead(pipeline);
        }
    } catch (Throwable t) {
        handleReadException(pipeline, byteBuf, t, close, allocHandle);
    } finally {
        // Check if there is a readPending which was not processed yet.
        // This could be for two reasons:
        // * The user called Channel.read() or ChannelHandlerContext.read() in channelRead(...) method
        // * The user called Channel.read() or ChannelHandlerContext.read() in channelReadComplete(...) method
        //
        // See https://github.com/netty/netty/issues/2254
        if (!readPending && !config.isAutoRead()) {
            removeReadOp();
        }
    }
}
}
```
`allocator = ByteBufAllocator.DEFAULT;`, 其默认的实例根据来定`allocType`来定:
```java
if ("unpooled".equals(allocType)) {
    alloc = UnpooledByteBufAllocator.DEFAULT;
    logger.debug("-Dio.netty.allocator.type: {}", allocType);
} else if ("pooled".equals(allocType)) {
    alloc = PooledByteBufAllocator.DEFAULT;
    logger.debug("-Dio.netty.allocator.type: {}", allocType);
} else {
    alloc = PooledByteBufAllocator.DEFAULT;
    logger.debug("-Dio.netty.allocator.type: pooled (unknown: {})", allocType);
}
```
传播`readRead`事件: 从Channel中读取数据到byteBuf中，如果数据大于ByteBuf的长度，则分多次读取；
传播`readComplete`事件: 从Channel中读取完数据之后，传播该事件。
接下来看`readRead`事件传播的逻辑，进入`io.netty.channel.DefaultChannelPipeline#fireChannelRead`
```java
@Override
public final ChannelPipeline fireChannelRead(Object msg) {
    AbstractChannelHandlerContext.invokeChannelRead(head, msg);
    return this;
}
```
msg为ByteBuf， 也就是每次读取的数据， 而head 为pipeline中的头`ChannelHandlerContext`-`AbstractChannelHandlerContext head;`, `invokeChannelRead`方法如下;
```java
static void invokeChannelRead(final AbstractChannelHandlerContext next, Object msg) {
    final Object m = next.pipeline.touch(ObjectUtil.checkNotNull(msg, "msg"), next);
    // 取出绑定当前Channel的EventLoop， next = context, 这个context依然是pipeline中的headContext
    EventExecutor executor = next.executor();
    if (executor.inEventLoop()) {
        // 因为当前执行线程就是EventLoop ， 所以继续执行这块的逻辑
        next.invokeChannelRead(m);
    } else {
        executor.execute(new Runnable() {
            @Override
            public void run() {
                next.invokeChannelRead(m);
            }
        });
    }
}
```
继续看invokeChannelRead操作，
```java
private void invokeChannelRead(Object msg) {
    // 是否需要执行Handler
    if (invokeHandler()) {
        try {
            // 获取绑定到当前Context中的ChannelHandler, 执行channelRead操作
            ((ChannelInboundHandler) handler()).channelRead(this, msg);
        } catch (Throwable t) {
            notifyHandlerException(t);
        }
    } else {
        fireChannelRead(msg);
    }
}
```
`invokeHandler()`校验是否需要执行Handler, 如果这个Context下的`ChannelHandler#handlerAdded`已经被调用过了，就不需要再调用了。
```java
private boolean invokeHandler() {
    // Store in local variable to reduce volatile reads.
    int handlerState = this.handlerState;
    return handlerState == ADD_COMPLETE || (!ordered && handlerState == ADD_PENDING);
}

Context 中handler的执行状态如下：
// 等待handlerAdded被调用
private static final int ADD_PENDING = 1;
// handlerAdded已经被调用
private static final int ADD_COMPLETE = 2;
// handlerRemoved已经被调用
private static final int REMOVE_COMPLETE = 3;
// 初始化状态
private static final int INIT = 0;

handlerAdded :  Gets called after the {@link ChannelHandler} was added to the actual context and it's ready to handle events.
当Handler被加入到具体的Context中，并且已经为处理事件准备就绪了，ChannelHandler#handlerAdded就会被调用。
```

`channelRead`操作会调用到下一个`handler`对应的方法上，那这个方法是我们自定义的`handler`中的吗？不是
我们在initChnanel过程中，加入了默认的ChannelHandler - `ServerBoostrapAcceptor`:
```java
ch.eventLoop().execute(new Runnable() {
    @Override
    public void run() {
        pipeline.addLast(new ServerBootstrapAcceptor(
                ch, currentChildGroup, currentChildHandler, currentChildOptions, currentChildAttrs));
    }
});
```
所以返回的handler 其实是`DefaultChannelPiple#HeadContext`, 这个`Context`内部的`ChannelHandler`就是如上代码中所述-`ServerBootstrapAcceptor`
接着调用`HeadContext`中的`channelRead`方法：
```java
@Override
public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception {
    ctx.fireChannelRead(msg);
}
```
不出意外，应该要进入ServerBootstrapAcceptor中的fireChannelRead方法了， 但是因为HeadContext并没有实现对应的方法，所以由它继承的AbstractChannelHandlerContext来执行
![](_v_images/20191116234617028_6187.png =800x)
抽象的ChannelHandlerContext实现如下:
```java
@Override
public ChannelHandlerContext fireChannelRead(final Object msg) {
    invokeChannelRead(findContextInbound(), msg);
    return this;
}
```
`findContextInbound()`执行如下:
```java
private AbstractChannelHandlerContext findContextInbound() {
    AbstractChannelHandlerContext ctx = this;
    do {
        // 获取ChannelHandlerContext的下一个inBoundChannelHandlerContext
        ctx = ctx.next;
    } while (!ctx.inbound);
    return ctx;
}
```
`invokeChannelRead()`操作如下:
```java
static void invokeChannelRead(final AbstractChannelHandlerContext next, Object msg) {
    final Object m = next.pipeline.touch(ObjectUtil.checkNotNull(msg, "msg"), next);
    EventExecutor executor = next.executor();
    if (executor.inEventLoop()) {
        next.invokeChannelRead(m);
    } else {
        executor.execute(new Runnable() {
            @Override
            public void run() {
                next.invokeChannelRead(m);
            }
        });
    }
}
```
又回到了刚才调用的地方，寻找到下一个channelHandlerContext之后，调用其内部绑定的Channelhandler，执行响应的逻辑。

## channelHandler调用结束
我们发现其实在ServerBootStrap中，我们添加了两个ChannelHander, 并且第二个Handler内部的对应方法，如-channelRead，我们没有在加fireChannelRead操作，这个时候就算是整个pipeline 中的inBoundChannelHandler都执行完了，那如果最后一个ChannelHandler, 仍然加fireChannelRead操作，会出现什么情况呢？
我们只需要做微修改就能验证-去掉其他的Channelhandler， 只保留第一个；
我们跟断点到达:
```java
@Override
public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception {
    ByteBuf in = (ByteBuf)msg;
    // 调用下一个inboundHandler处理
    System.out.println("第一个inBoundhandler获得处理");
    ctx.fireChannelRead(msg);
}
```
继续走进去之后，我们发现最终寻找下一个Context时，返回了-TailContext
```java
private AbstractChannelHandlerContext findContextInbound() {
    AbstractChannelHandlerContext ctx = this;
    do {
        ctx = ctx.next;
    } while (!ctx.inbound);
    return ctx;
}
```
![](_v_images/20191117000905785_27206.png =800x)
并且这个Context.next =null, 既然已经返回了Context， 我们看它对这个事件最终是如何处理的：
```java
static void invokeChannelRead(final AbstractChannelHandlerContext next, Object msg) {
    final Object m = next.pipeline.touch(ObjectUtil.checkNotNull(msg, "msg"), next);
    EventExecutor executor = next.executor();
    if (executor.inEventLoop()) {
        next.invokeChannelRead(m);
    } else {
        executor.execute(new Runnable() {
            @Override
            public void run() {
                next.invokeChannelRead(m);
            }
        });
    }
}
```
最终调用的TailContext的如下方法：
```java
@Override
public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception {
    onUnhandledInboundMessage(msg);
}
```
接下来，在debug之后，之后释放了这个缓冲区，整个流程就算完成了。
```java
protected void onUnhandledInboundMessage(Object msg) {
    try {
        logger.debug(
                "Discarded inbound message {} that reached at the tail of the pipeline. " +
                        "Please check your pipeline configuration.", msg);
    } finally {
        ReferenceCountUtil.release(msg);
    }
}
```

## Unsafe
![Unsafe](_v_images/20191214222621956_23857.png =500x)
我们发现，Server在接收客户端的连接时，对应read操作是执行NioMessageUnsafe.read，而真正在执行read读取客户端消息的时候，是执行NioByteUnsafe.read。
我们看下unsafe的由来：
```java
private void processSelectedKey(SelectionKey k, AbstractNioChannel ch) {
    final AbstractNioChannel.NioUnsafe unsafe = ch.unsafe();
    ...
}
```
我们看下外部的调用, 看样子是从selectionKey中携带过来的。AbstractNioChannel = selectedKey.attachment();
```java
private void processSelectedKeysPlain(Set<SelectionKey> selectedKeys) {
    Iterator<SelectionKey> i = selectedKeys.iterator();
    for (;;) {
        final SelectionKey k = i.next();
        final Object a = k.attachment();
        i.remove();
        if (a instanceof AbstractNioChannel) {
            processSelectedKey(k, (AbstractNioChannel) a);
        }
    }
}
```
我们可以猜到大致原因是NioServerSocketChannel和NioSocketChannel所返回的Unsafe不一致导致的。
### ServerChannel
![](_v_images/20191214225903543_18598.png =600x)
NioServerSocketChannel没有unsafe方法，但是他继承的父类有unsafe方法，但是这个unsafe却是在构造NioServerSocketChannel实例的过程中生成，我们看,以下是NioServerSocketChannel的构造方法：
```java
/**
 * Create a new instance
 */
public NioServerSocketChannel() {
    this(newSocket(DEFAULT_SELECTOR_PROVIDER));
}
```
往上一直看就会发现AbstractChannel的构造方法中，初始化了unsafe实例：
```java
protected AbstractChannel(Channel parent) {
    this.parent = parent;
    id = newId();
    unsafe = newUnsafe();
    pipeline = newChannelPipeline();
}
```
而这个newUnsafe最终是在AbstractNioMessageChannel中实现:
```java
io.netty.channel.nio.AbstractNioMessageChannel#newUnsafe
@Override
protected AbstractNioUnsafe newUnsafe() {
    return new NioMessageUnsafe();
}
```
这就解析了，为什么在服务端接收新连接时，调用的是NioMessageUnsafe的read方法。
### Channel
同样的，我们继续看NioSocketChannel的继承图：
![](_v_images/20191214230341133_28273.png =600x)
按照之前的经验，我们直接看NioSocketChannel的构造函数：
```java
/**
 * Create a new instance
 */
public NioSocketChannel() {
    this(DEFAULT_SELECTOR_PROVIDER);
}
```
我们同样发现了其继承的顶层父类在构造unsafe的时候同样用到了newUnsafe(), 我们再看NioSocketChannel的父类AbstractNioByteChannel，它在调用newUnsafe的时候, 实例化了NioByteUnsafe， 区别就在于此了。
```java
@Override
protected AbstractNioUnsafe newUnsafe() {
    return new NioByteUnsafe();
}
```

### 疑问
1. 如果某个channel关闭了的话, 关闭了之后EventLoop需要做什么(?) ;
2. Channel交接的过程中，bossEventLoop在调用workEventLoop注册之后，它是不是要释放对该channel的注册，否则当该Channel上有事件时，岂不是两个EventLoop都会被触发？
在实例化NioServerSocketChannel以及NioSocketChannel时如下：
```java
----------------NioServerSocketChannel
public NioServerSocketChannel(ServerSocketChannel channel) {
    super(null, channel, SelectionKey.OP_ACCEPT);
    config = new NioServerSocketChannelConfig(this, javaChannel().socket());
}
----------------NioSocketChannel
protected AbstractNioByteChannel(Channel parent, SelectableChannel ch) {
    super(parent, ch, SelectionKey.OP_READ);
}
```

### 总结
服务端在接收新连接以及与客户端交互的过程中，使用了不同的read操作，但是大致来看，接收新连接仅仅是将channel往后传播，具体的操作是交给了ServerBootstrapAcceptor来处理，从而在channelRead时，将channel注册到了workGroup中；而真正的交互交给了workGroup，由它去处理已经注册好的连接的相应事件，而workGroup处理的是NioSocketChannel，所以在客户端发送数据到服务端时，服务端对应的read操作是读取数据，再往后传播事件，这里面涉及到一些粘包/拆包等与具体数据有关的操作。并且NioServerSocketChannel与NioSocketChannel的pipeLine不一样，其中对应的Channelhandler也不一致，NioServerSocketChannel所匹配的Pipeline中handler如： HeadContext -> ServerAcceptor -> TailContext , 而NioSocketChannel中pipeLine的handler如： HeadContext ->...(用户自定义的Handler)...->TailContext。
